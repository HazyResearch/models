DISPLAY "(null)" invalid; disabling X11 forwarding
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
INFO:tensorflow:PS hosts are: ['raiders1:2228']
INFO:tensorflow:Worker hosts are: ['raiders1:2200', 'raiders1:2201', 'raiders3:2209', 'raiders3:2210']
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:43:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:43:00.0)
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {raiders1:2228}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {raiders1:2200, raiders1:2201, raiders3:2209, localhost:2210}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2210
INFO:tensorflow:Learning rate: 0.000500, momentum: 0.900000
INFO:tensorflow:2016-07-21 03:19:03.766100 Supervisor
INFO:tensorflow:Started 3 queues for processing input data.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3576 get requests, put_count=2565 evicted_count=1000 eviction_rate=0.389864 and unsatisfied allocation rate=0.590324
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
INFO:tensorflow:Step: 418, Loss: 4.989534, time: 11.064 sec/batch
INFO:tensorflow:Step: 419, Loss: 5.139236, time: 1.281 sec/batch
INFO:tensorflow:Step: 421, Loss: 5.179118, time: 1.262 sec/batch
INFO:tensorflow:Step: 423, Loss: 5.093737, time: 2.084 sec/batch
INFO:tensorflow:Step: 425, Loss: 5.063566, time: 1.325 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1016 evicted_count=1000 eviction_rate=0.984252 and unsatisfied allocation rate=0
INFO:tensorflow:Step: 427, Loss: 5.149249, time: 1.279 sec/batch
INFO:tensorflow:Step: 429, Loss: 5.157977, time: 1.335 sec/batch
INFO:tensorflow:Step: 431, Loss: 4.984497, time: 1.332 sec/batch
INFO:tensorflow:Step: 433, Loss: 5.164716, time: 1.491 sec/batch
INFO:tensorflow:Step: 435, Loss: 5.294711, time: 1.417 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1025 evicted_count=1000 eviction_rate=0.97561 and unsatisfied allocation rate=0
INFO:tensorflow:Step: 437, Loss: 4.927092, time: 1.276 sec/batch
INFO:tensorflow:Step: 439, Loss: 5.008587, time: 1.340 sec/batch
INFO:tensorflow:Step: 441, Loss: 5.083234, time: 1.305 sec/batch
INFO:tensorflow:Step: 443, Loss: 4.964688, time: 1.254 sec/batch
INFO:tensorflow:Step: 445, Loss: 5.145332, time: 1.324 sec/batch
INFO:tensorflow:Step: 447, Loss: 4.891752, time: 1.307 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1044 evicted_count=1000 eviction_rate=0.957854 and unsatisfied allocation rate=0
INFO:tensorflow:Step: 449, Loss: 4.889856, time: 1.298 sec/batch
INFO:tensorflow:Step: 451, Loss: 4.840577, time: 1.310 sec/batch
INFO:tensorflow:Step: 453, Loss: 5.010622, time: 1.348 sec/batch
INFO:tensorflow:Step: 455, Loss: 5.077471, time: 1.315 sec/batch
INFO:tensorflow:Step: 457, Loss: 4.931349, time: 1.521 sec/batch
INFO:tensorflow:Step: 459, Loss: 5.158985, time: 1.439 sec/batch
INFO:tensorflow:Step: 461, Loss: 5.036895, time: 1.317 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3575 get requests, put_count=4411 evicted_count=2000 eviction_rate=0.453412 and unsatisfied allocation rate=0.347692
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 871 to 958
INFO:tensorflow:Step: 463, Loss: 5.248201, time: 1.269 sec/batch
INFO:tensorflow:Step: 465, Loss: 4.977733, time: 1.299 sec/batch
INFO:tensorflow:Step: 467, Loss: 4.847487, time: 1.303 sec/batch
INFO:tensorflow:Step: 469, Loss: 4.947321, time: 1.235 sec/batch
INFO:tensorflow:Step: 471, Loss: 5.121757, time: 1.302 sec/batch
not cropping. Imagesize: 256
not cropping. Imagesize: 256
not cropping. Imagesize: 256
not cropping. Imagesize: 256
Traceback (most recent call last):
  File "/afs/cs.stanford.edu/u/daniter/tf/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py", line 69, in <module>
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 30, in run
    sys.exit(main(sys.argv))
  File "/afs/cs.stanford.edu/u/daniter/tf/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py", line 65, in main
    inception_distributed_train.train(server.target, dataset, cluster_spec)
  File "/afs/cs.stanford.edu/u/daniter/tf/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py", line 298, in train
    assert not np.isnan(loss_value), 'Model diverged with loss = NaN'
AssertionError: Model diverged with loss = NaN
