I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
INFO:tensorflow:PS hosts are: ['master:2200']
INFO:tensorflow:Worker hosts are: ['master:2201', 'master:2202', 'master:2203', 'master:2204', 'node001:2205', 'node001:2206', 'node001:2207', 'node001:2208', 'node002:2209', 'node002:2210', 'node002:2211', 'node002:2212', 'node003:2213', 'node003:2214', 'node003:2215', 'node003:2216', 'node004:2217', 'node004:2218', 'node004:2219', 'node004:2220', 'node005:2221', 'node005:2222', 'node005:2223', 'node005:2224', 'node006:2225', 'node006:2226', 'node006:2227', 'node006:2228', 'node007:2229', 'node007:2230', 'node007:2231', 'node007:2232']
I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:922] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GRID K520
major: 3 minor: 0 memoryClockRate (GHz) 0.797
pciBusID 0000:00:03.0
Total memory: 4.00GiB
Free memory: 3.95GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:784] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GRID K520, pci bus id: 0000:00:03.0)
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {master:2200}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {master:2201, master:2202, master:2203, master:2204, node001:2205, node001:2206, node001:2207, node001:2208, node002:2209, node002:2210, node002:2211, node002:2212, node003:2213, node003:2214, node003:2215, node003:2216, node004:2217, node004:2218, node004:2219, node004:2220, node005:2221, node005:2222, node005:2223, node005:2224, node006:2225, node006:2226, node006:2227, node006:2228, localhost:2229, node007:2230, node007:2231, node007:2232}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2229
INFO:tensorflow:Learning rate: 0.010000, momentum: 0.900000
INFO:tensorflow:Sync mode!!!!!!
INFO:tensorflow:Compute groups : 1
INFO:tensorflow:SyncReplicas enabled: replicas_to_aggregate=32; total_num_replicas=32
INFO:tensorflow:Num nodes per compute groups: 32
INFO:tensorflow:Tokens per step: 32
INFO:tensorflow:2016-08-10 06:25:05.973278 Supervisor
INFO:tensorflow:Waiting for model to be ready: Variables not initialized: mixed_35x35x256a/branch5x5/Conv/BatchNorm/moving_mean, mixed_35x35x256a/branch5x5/Conv/BatchNorm/moving_variance, mixed_35x35x256a/branch5x5/Conv_1/BatchNorm/moving_mean, mixed_35x35x256a/branch5x5/Conv_1/BatchNorm/moving_variance, mixed_35x35x256a/branch3x3dbl/Conv/BatchNorm/moving_mean, mixed_35x35x256a/branch3x3dbl/Conv_1/BatchNorm/moving_mean, mixed_35x35x256a/branch3x3dbl/Conv_2/BatchNorm/beta, mixed_35x35x288a/branch1x1/Conv/weights, mixed_35x35x288a/branch1x1/Conv/BatchNorm/moving_variance, mixed_35x35x288a/branch5x5/Conv/weights, mixed_35x35x288a/branch3x3dbl/Conv/weights, mixed_35x35x288a/branch3x3dbl/Conv/BatchNorm/moving_variance, mixed_35x35x288a/branch3x3dbl/Conv_2/weights, mixed_35x35x288b/branch5x5/Conv/BatchNorm/moving_mean, mixed_35x35x288b/branch3x3dbl/Conv_2/BatchNorm/moving_mean, mixed_35x35x288b/branch_pool/Conv/BatchNorm/beta, mixed_17x17x768a/branch3x3dbl/Conv/BatchNorm/moving_variance, mixed_17x17x768a/branch3x3dbl/Conv_1/BatchNorm/beta, mixed_17x17x768a/branch3x3dbl/Conv_1/BatchNorm/moving_mean, mixed_17x17x768a/branch3x3dbl/Conv_1/BatchNorm/moving_variance, mixed_17x17x768b/branch1x1/Conv/weights, mixed_17x17x768b/branch1x1/Conv/BatchNorm/moving_variance, mixed_17x17x768b/branch7x7/Conv/weights, mixed_17x17x768b/branch7x7/Conv/BatchNorm/moving_variance, mixed_17x17x768b/branch7x7/Conv_1/BatchNorm/moving_mean, mixed_17x17x768b/branch7x7/Conv_2/weights, mixed_17x17x768b/branch7x7dbl/Conv/weights, mixed_17x17x768b/branch7x7dbl/Conv_1/BatchNorm/moving_variance, mixed_17x17x768b/branch7x7dbl/Conv_2/weights, mixed_17x17x768b/branch7x7dbl/Conv_3/weights, mixed_17x17x768b/branch_pool/Conv/weights, mixed_17x17x768c/branch1x1/Conv/weights, mixed_17x17x768c/branch7x7dbl/Conv_3/BatchNorm/moving_mean, mixed_17x17x768d/branch1x1/Conv/BatchNorm/moving_mean, mixed_17x17x768d/branch1x1/Conv/BatchNorm/moving_variance, mixed_17x17x768d/branch7x7/Conv/BatchNorm/beta, mixed_17x17x768d/branch7x7/Conv_1/BatchNorm/beta, mixed_17x17x768d/branch7x7/Conv_2/weights, mixed_17x17x768d/branch7x7/Conv_2/BatchNorm/beta, mixed_17x17x768d/branch7x7/Conv_2/BatchNorm/moving_variance, mixed_17x17x768d/branch7x7dbl/Conv_1/BatchNorm/beta, mixed_17x17x768d/branch7x7dbl/Conv_2/weights, mixed_17x17x768d/branch7x7dbl/Conv_2/BatchNorm/beta, mixed_17x17x768e/branch1x1/Conv/BatchNorm/beta, mixed_17x17x768e/branch7x7/Conv_1/BatchNorm/moving_variance, mixed_17x17x768e/branch7x7dbl/Conv/weights, mixed_17x17x768e/branch7x7dbl/Conv_1/weights, aux_logits/Conv/weights, mixed_8x8x2048a/branch1x1/Conv/BatchNorm/moving_variance, mixed_8x8x2048a/branch3x3/Conv/BatchNorm/moving_mean, mixed_8x8x2048a/branch3x3dbl/Conv_3/weights, mixed_8x8x2048b/branch3x3dbl/Conv_3/weights, mixed_35x35x256a/branch3x3dbl/Conv_1/weights/Momentum, aux_logits/Conv/weights/Momentum, mixed_8x8x2048b/branch3x3/Conv/weights/Momentum, logits/logits/weights/Momentum, aux_logits/Conv/weights/ExponentialMovingAverage, mixed_8x8x2048b/branch3x3/Conv_1/weights/ExponentialMovingAverage, mixed_17x17x768d/branch7x7dbl/Conv_4/BatchNorm/moving_mean/ExponentialMovingAverage, mixed_17x17x768d/branch_pool/Conv/BatchNorm/moving_mean/ExponentialMovingAverage, mixed_17x17x768e/branch7x7/Conv/BatchNorm/moving_mean/ExponentialMovingAverage, mixed_17x17x768e/branch7x7/Conv/BatchNorm/moving_variance/ExponentialMovingAverage, mixed_17x17x768e/branch7x7/Conv_2/BatchNorm/moving_mean/ExponentialMovingAverage
INFO:tensorflow:got sessions! 2016-08-10 06:26:20.890598 
INFO:tensorflow:Started 3 queues for processing input data.
W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 830.43MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
W tensorflow/core/common_runtime/bfc_allocator.cc:213] Ran out of memory trying to allocate 830.43MiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3769 get requests, put_count=2856 evicted_count=1000 eviction_rate=0.35014 and unsatisfied allocation rate=0.534094
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
INFO:tensorflow:Step: 7871, Loss: 9.570735, time: 32.654 sec/batch
INFO:tensorflow:Step: 7872, Loss: 8.966602, time: 16.749 sec/batch
INFO:tensorflow:Step: 7874, Loss: 9.251691, time: 12.794 sec/batch
INFO:tensorflow:Step: 7875, Loss: 10.048472, time: 14.218 sec/batch
INFO:tensorflow:Step: 7876, Loss: 9.554292, time: 11.556 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1016 evicted_count=1000 eviction_rate=0.984252 and unsatisfied allocation rate=0
INFO:tensorflow:Step: 7877, Loss: 9.490408, time: 12.139 sec/batch
INFO:tensorflow:Step: 7878, Loss: 9.927107, time: 12.512 sec/batch
INFO:tensorflow:Step: 7879, Loss: 9.257252, time: 12.469 sec/batch
INFO:tensorflow:Step: 7880, Loss: 9.054833, time: 12.580 sec/batch
INFO:tensorflow:Step: 7881, Loss: 9.489696, time: 12.727 sec/batch
INFO:tensorflow:Step: 7882, Loss: 9.715619, time: 12.440 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3768 get requests, put_count=4060 evicted_count=2000 eviction_rate=0.492611 and unsatisfied allocation rate=0.459926
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 281 to 309
INFO:tensorflow:Step: 7883, Loss: 9.019517, time: 12.899 sec/batch
INFO:tensorflow:Step: 7884, Loss: 8.882423, time: 13.239 sec/batch
INFO:tensorflow:Step: 7885, Loss: 9.827614, time: 12.544 sec/batch
INFO:tensorflow:Step: 7886, Loss: 9.095224, time: 13.072 sec/batch
INFO:tensorflow:Step: 7887, Loss: 9.130989, time: 11.722 sec/batch
INFO:tensorflow:Step: 7888, Loss: 9.937643, time: 10.772 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1049 evicted_count=1000 eviction_rate=0.953289 and unsatisfied allocation rate=0
INFO:tensorflow:Step: 7889, Loss: 9.663755, time: 12.507 sec/batch
INFO:tensorflow:Step: 7890, Loss: 8.972969, time: 13.313 sec/batch
INFO:tensorflow:Step: 7891, Loss: 9.475847, time: 14.170 sec/batch
INFO:tensorflow:Step: 7892, Loss: 9.551592, time: 12.560 sec/batch
INFO:tensorflow:Step: 7893, Loss: 8.741386, time: 11.500 sec/batch
INFO:tensorflow:Step: 7894, Loss: 9.871086, time: 12.258 sec/batch
INFO:tensorflow:Step: 7895, Loss: 9.853919, time: 11.818 sec/batch
INFO:tensorflow:Step: 7896, Loss: 8.815060, time: 14.296 sec/batch
INFO:tensorflow:Step: 7897, Loss: 9.283349, time: 12.829 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3768 get requests, put_count=4015 evicted_count=1000 eviction_rate=0.249066 and unsatisfied allocation rate=0.227707
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 1158 to 1273
INFO:tensorflow:Step: 7898, Loss: 9.096922, time: 13.273 sec/batch
INFO:tensorflow:Step: 7899, Loss: 9.348179, time: 13.574 sec/batch
INFO:tensorflow:Step: 7900, Loss: 9.162543, time: 13.523 sec/batch
INFO:tensorflow:Step: 7901, Loss: 9.265289, time: 15.015 sec/batch
INFO:tensorflow:Step: 7902, Loss: 9.491595, time: 12.681 sec/batch
INFO:tensorflow:Step: 7903, Loss: 9.440336, time: 11.009 sec/batch
INFO:tensorflow:Step: 7904, Loss: 9.478268, time: 11.942 sec/batch
INFO:tensorflow:Step: 7905, Loss: 9.046061, time: 13.184 sec/batch
INFO:tensorflow:Step: 7906, Loss: 9.052965, time: 12.648 sec/batch
INFO:tensorflow:Step: 7907, Loss: 9.756258, time: 11.968 sec/batch
INFO:tensorflow:Step: 7908, Loss: 9.273413, time: 14.443 sec/batch
E tensorflow/core/distributed_runtime/master_session.cc:923] Cleanup partition error: Unavailable: 
Traceback (most recent call last):
  File "/root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py", line 69, in <module>
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 30, in run
    sys.exit(main(sys.argv))
  File "/root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py", line 65, in main
    inception_distributed_train.train(server.target, dataset, cluster_spec)
  File "/root/models/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py", line 330, in train
    loss_value, step = sess.run([train_op, global_step])
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 333, in run
    run_metadata_ptr)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 573, in _run
    feed_dict_string, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 648, in _do_run
    target_list, options, run_metadata)
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.py", line 668, in _do_call
    raise type(e)(node_def, op, message)
tensorflow.python.framework.errors.UnavailableError
