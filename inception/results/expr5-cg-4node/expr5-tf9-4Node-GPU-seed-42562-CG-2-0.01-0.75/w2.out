I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcublas.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcudnn.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcufft.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcuda.so locally
I tensorflow/stream_executor/dso_loader.cc:108] successfully opened CUDA library libcurand.so locally
INFO:tensorflow:PS hosts are: ['raiders1:2228']
INFO:tensorflow:Worker hosts are: ['raiders1:2200', 'raiders1:2201', 'raiders3:2209', 'raiders3:2210']
I tensorflow/core/common_runtime/gpu/gpu_init.cc:102] Found device 0 with properties: 
name: GeForce GTX TITAN X
major: 5 minor: 2 memoryClockRate (GHz) 1.076
pciBusID 0000:43:00.0
Total memory: 11.92GiB
Free memory: 11.81GiB
I tensorflow/core/common_runtime/gpu/gpu_init.cc:126] DMA: 0 
I tensorflow/core/common_runtime/gpu/gpu_init.cc:136] 0:   Y 
I tensorflow/core/common_runtime/gpu/gpu_device.cc:806] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX TITAN X, pci bus id: 0000:43:00.0)
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job ps -> {raiders1:2228}
I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:206] Initialize HostPortsGrpcChannelCache for job worker -> {raiders1:2200, localhost:2201, raiders3:2209, raiders3:2210}
I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:202] Started server with target: grpc://localhost:2201
INFO:tensorflow:Learning rate: 0.010000, momentum: 0.750000
INFO:tensorflow:Sync mode!!!!!!
INFO:tensorflow:Compute groups : 2
INFO:tensorflow:SyncReplicas enabled: replicas_to_aggregate=4; total_num_replicas=4
INFO:tensorflow:Num nodes per compute groups: 2
INFO:tensorflow:Tokens per step: 2
INFO:tensorflow:2016-07-19 03:19:51.649403 Supervisor
INFO:tensorflow:got sessions!
INFO:tensorflow:Started 3 queues for processing input data.
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3576 get requests, put_count=2565 evicted_count=1000 eviction_rate=0.389864 and unsatisfied allocation rate=0.590324
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 100 to 110
INFO:tensorflow:Step: 399, Accuracy: 0.625000, Loss: 5.250357, time: 6.937 sec/batch
INFO:tensorflow:Step: 399, Accuracy: 0.609375, Loss: 5.611794, time: 1.155 sec/batch
INFO:tensorflow:Step: 400, Accuracy: 0.562500, Loss: 5.106387, time: 1.270 sec/batch
INFO:tensorflow:Step: 400, Accuracy: 0.531250, Loss: 5.517705, time: 1.177 sec/batch
INFO:tensorflow:Step: 401, Accuracy: 0.531250, Loss: 5.321858, time: 1.163 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1016 evicted_count=1000 eviction_rate=0.984252 and unsatisfied allocation rate=0
INFO:tensorflow:Step: 401, Accuracy: 0.656250, Loss: 5.375739, time: 1.276 sec/batch
INFO:tensorflow:Step: 402, Accuracy: 0.562500, Loss: 5.335567, time: 1.230 sec/batch
INFO:tensorflow:Step: 403, Accuracy: 0.531250, Loss: 5.671104, time: 1.276 sec/batch
INFO:tensorflow:Step: 404, Accuracy: 0.531250, Loss: 5.182068, time: 1.315 sec/batch
INFO:tensorflow:Step: 405, Accuracy: 0.562500, Loss: 5.426187, time: 1.266 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1025 evicted_count=1000 eviction_rate=0.97561 and unsatisfied allocation rate=0
INFO:tensorflow:Step: 406, Accuracy: 0.640625, Loss: 5.268349, time: 1.264 sec/batch
INFO:tensorflow:Step: 407, Accuracy: 0.718750, Loss: 5.232144, time: 1.291 sec/batch
INFO:tensorflow:Step: 408, Accuracy: 0.531250, Loss: 5.416797, time: 1.467 sec/batch
INFO:tensorflow:Step: 409, Accuracy: 0.546875, Loss: 5.399100, time: 1.245 sec/batch
INFO:tensorflow:Step: 410, Accuracy: 0.562500, Loss: 5.045057, time: 1.334 sec/batch
INFO:tensorflow:Step: 411, Accuracy: 0.640625, Loss: 5.157251, time: 1.678 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 0 get requests, put_count=1044 evicted_count=1000 eviction_rate=0.957854 and unsatisfied allocation rate=0
INFO:tensorflow:Step: 411, Accuracy: 0.593750, Loss: 5.372888, time: 1.533 sec/batch
INFO:tensorflow:Step: 412, Accuracy: 0.640625, Loss: 5.308119, time: 1.159 sec/batch
INFO:tensorflow:Step: 413, Accuracy: 0.625000, Loss: 5.241471, time: 1.223 sec/batch
INFO:tensorflow:Step: 414, Accuracy: 0.593750, Loss: 5.463973, time: 1.222 sec/batch
INFO:tensorflow:Step: 415, Accuracy: 0.578125, Loss: 5.376008, time: 1.280 sec/batch
INFO:tensorflow:Step: 416, Accuracy: 0.656250, Loss: 5.159777, time: 1.379 sec/batch
INFO:tensorflow:Step: 417, Accuracy: 0.531250, Loss: 5.320959, time: 1.347 sec/batch
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:244] PoolAllocator: After 3579 get requests, put_count=4416 evicted_count=2000 eviction_rate=0.452899 and unsatisfied allocation rate=0.347024
I tensorflow/core/common_runtime/gpu/pool_allocator.cc:256] Raising pool_size_limit_ from 871 to 958
INFO:tensorflow:Step: 418, Accuracy: 0.671875, Loss: 5.301251, time: 1.390 sec/batch
INFO:tensorflow:Step: 419, Accuracy: 0.609375, Loss: 5.269063, time: 1.534 sec/batch
INFO:tensorflow:Step: 420, Accuracy: 0.000000, Loss: 5.378194, time: 1.501 sec/batch
not cropping. Imagesize: 256
not cropping. Imagesize: 256
not cropping. Imagesize: 256
not cropping. Imagesize: 256
Traceback (most recent call last):
  File "/afs/cs.stanford.edu/u/daniter/tf-test/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py", line 69, in <module>
    tf.app.run()
  File "/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py", line 30, in run
    sys.exit(main(sys.argv))
  File "/afs/cs.stanford.edu/u/daniter/tf-test/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/imagenet_distributed_train.py", line 65, in main
    inception_distributed_train.train(server.target, dataset, cluster_spec)
  File "/afs/cs.stanford.edu/u/daniter/tf-test/inception/bazel-bin/inception/imagenet_distributed_train.runfiles/__main__/inception/inception_distributed_train.py", line 315, in train
    assert not np.isnan(loss_value), 'Model diverged with loss = NaN'
AssertionError: Model diverged with loss = NaN
